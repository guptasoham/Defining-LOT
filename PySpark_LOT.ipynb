{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Spark, and Findspark"
      ],
      "metadata": {
        "id": "VKzKp8jnfipA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing pyspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "gXJ3sT7836jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1513340e-4fdf-42a0-cc23-2a91607c7787"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=343aa64d2e8b6340e1dd526e95d0b6222d0c743565a0e29bb4fd454db85a7d3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "BiuIx_Bt1jrJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary packages"
      ],
      "metadata": {
        "id": "b_lMYBRI4W0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, when, datediff, lit, concat_ws, collect_set, min, max, element_at, expr\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import glob"
      ],
      "metadata": {
        "id": "8ylwE0dc4VnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing Spark session"
      ],
      "metadata": {
        "id": "A2F7KgNyfxsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "U7I1javk6uFm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PySpark code"
      ],
      "metadata": {
        "id": "Gqkb0_4WgCFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CancerDataAnalysis\").getOrCreate()\n",
        "\n",
        "# At the beginning of your script, after creating the SparkSession:\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "# Read the CSV file\n",
        "data = spark.read.csv(\"/content/drive/MyDrive/CareSet-Solution/Files/CancerData0.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Select needed columns\n",
        "needed_columns = ['patient_ID', 'drug_date', 'drug', 'days_of_supply', 'class', 'drug_start_date', 'drug_end_date']\n",
        "cancer_data = data.select(needed_columns)\n",
        "\n",
        "# Convert date columns\n",
        "cancer_data = cancer_data.withColumn(\"drug_date\", to_date(col(\"drug_date\"), \"dd-MMM-yy\"))\n",
        "cancer_data = cancer_data.withColumn(\"drug_start_date\", to_date(col(\"drug_start_date\"), \"M/d/yy\"))\n",
        "cancer_data = cancer_data.withColumn(\"drug_end_date\", to_date(col(\"drug_end_date\"), \"M/d/yy\"))\n",
        "\n",
        "# Replace '.' with 0 in days_of_supply and convert to integer\n",
        "cancer_data = cancer_data.withColumn(\"days_of_supply\",\n",
        "    when(col(\"days_of_supply\") == \".\", 0).otherwise(col(\"days_of_supply\")).cast(\"int\"))\n",
        "\n",
        "# Handle missing start and end dates\n",
        "cancer_data = cancer_data.withColumn(\"drug_start_date\",\n",
        "    when((col(\"days_of_supply\") == 0) & col(\"drug_start_date\").isNull(), col(\"drug_date\"))\n",
        "    .when((col(\"days_of_supply\") != 0) & col(\"drug_start_date\").isNull(), col(\"drug_date\"))\n",
        "    .otherwise(col(\"drug_start_date\")))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"drug_end_date\",\n",
        "    when((col(\"days_of_supply\") == 0) & col(\"drug_end_date\").isNull(), col(\"drug_date\"))\n",
        "    .when((col(\"days_of_supply\") != 0) & col(\"drug_end_date\").isNull(),\n",
        "          expr('date_add(drug_date, days_of_supply)'))\n",
        "    .otherwise(col(\"drug_end_date\")))\n",
        "\n",
        "# Check and adjust drug_end_date\n",
        "cancer_data = cancer_data.withColumn(\"drug_end_date\",\n",
        "    when(datediff(col(\"drug_end_date\"), col(\"drug_start_date\")) != col(\"days_of_supply\"),\n",
        "         expr('date_add(drug_date, days_of_supply)'))\n",
        "    .otherwise(col(\"drug_end_date\")))\n",
        "\n",
        "# Sort the data\n",
        "cancer_data = cancer_data.orderBy(\"patient_ID\", \"drug_start_date\", \"drug_end_date\")\n",
        "\n",
        "# Define window for lag operations\n",
        "window = Window.partitionBy(\"patient_ID\").orderBy(\"drug_start_date\")\n",
        "\n",
        "# Calculate gap and add lot_num\n",
        "cancer_data = cancer_data.withColumn(\"prev_end_date\", F.lag(\"drug_end_date\").over(window))\n",
        "cancer_data = cancer_data.withColumn(\"gap\",\n",
        "    when(col(\"prev_end_date\").isNull(), 0)\n",
        "    .otherwise(datediff(col(\"drug_start_date\"), col(\"prev_end_date\"))))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"lot_num\",\n",
        "    F.sum(when(col(\"gap\") > 45, 1).otherwise(0)).over(window) + 1)\n",
        "\n",
        "# Calculate type1, type2, flag, and type\n",
        "cancer_data = cancer_data.withColumn(\"lot_start_date\",\n",
        "    F.first(\"drug_start_date\").over(Window.partitionBy(\"patient_ID\", \"lot_num\")))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"type1\",\n",
        "    when(col(\"gap\") > 45, 1).otherwise(0))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"type2\",\n",
        "    when((datediff(col(\"drug_start_date\"), col(\"lot_start_date\")) <= 29) &\n",
        "         (~F.lag(\"class\").over(window).contains(col(\"class\"))), 1)\n",
        "    .otherwise(0))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"flag\",\n",
        "    when((col(\"lot_num\") > 1) | (col(\"type2\") == 1), 1).otherwise(0))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"type\",\n",
        "    when((col(\"lot_num\") > 1) | (col(\"type2\") == 1), 1).otherwise(0))\n",
        "\n",
        "# Calculate cumulative drug names\n",
        "cancer_data = cancer_data.withColumn(\"cumulative_drug_names\",\n",
        "    F.collect_set(\"drug\").over(Window.partitionBy(\"patient_ID\", \"lot_num\")\n",
        "                               .orderBy(\"drug_start_date\")\n",
        "                               .rangeBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "cancer_data = cancer_data.withColumn(\"cumulative_drug_names\",\n",
        "    concat_ws(\" + \", F.sort_array(\"cumulative_drug_names\")))\n",
        "\n",
        "def save_as_single_csv(df, output_path):\n",
        "    # Save to a temporary directory\n",
        "    temp_dir = output_path + \"_temp\"\n",
        "\n",
        "    # Check if the temporary directory exists and remove it if it does\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)  # Remove the existing directory\n",
        "\n",
        "    # Use coalesce(1) to ensure a single partition and set maxRecordsPerFile to a large number\n",
        "    df.coalesce(1).write.option(\"header\", \"true\").csv(temp_dir)\n",
        "\n",
        "    # Find all CSV files in the temp directory (excluding _SUCCESS file)\n",
        "    csv_files = [f for f in glob.glob(os.path.join(temp_dir, \"*.csv\")) if not f.endswith(\"_SUCCESS\")]\n",
        "\n",
        "    if csv_files:\n",
        "        # Read and combine all CSV files\n",
        "        combined_df = pd.concat([pd.read_csv(f) for f in csv_files])\n",
        "\n",
        "        # Sort the combined DataFrame\n",
        "        if 'patient_ID' in combined_df.columns and 'drug_start_date' in combined_df.columns:\n",
        "            combined_df['drug_start_date'] = pd.to_datetime(combined_df['drug_start_date'])\n",
        "            combined_df = combined_df.sort_values(['patient_ID', 'drug_start_date'])\n",
        "        elif 'patient_ID' in combined_df.columns and 'line_start_date' in combined_df.columns:\n",
        "            combined_df['line_start_date'] = pd.to_datetime(combined_df['line_start_date'])\n",
        "            combined_df = combined_df.sort_values(['patient_ID', 'line_start_date'])\n",
        "\n",
        "        # Write the combined DataFrame to a single CSV file\n",
        "        combined_df.to_csv(output_path, index=False)\n",
        "\n",
        "        # Remove the temporary directory\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "        print(f\"File saved: {output_path}\")\n",
        "    else:\n",
        "        print(\"No CSV files found in the temporary directory.\")\n",
        "\n",
        "# sorting output data by date and patient id\n",
        "cancer_data = cancer_data.orderBy(\"patient_ID\", \"drug_start_date\")\n",
        "\n",
        "# Save output data to a single CSV file\n",
        "output_path = \"output_data.csv\"\n",
        "save_as_single_csv(cancer_data, output_path)\n",
        "\n",
        "# Create summary data\n",
        "summary_data = cancer_data.groupBy(\"patient_ID\", \"lot_num\").agg(\n",
        "    F.min(\"drug_start_date\").alias(\"line_start_date\"),\n",
        "    concat_ws(\" + \", F.collect_set(\"drug\")).alias(\"line_regimen\"),\n",
        "    concat_ws(\" + \", F.collect_set(\"class\")).alias(\"trgt_category_regimen\")\n",
        ")\n",
        "\n",
        "summary_data = summary_data.withColumnRenamed(\"lot_num\", \"line_num\")\n",
        "\n",
        "# sorting summary data by date and patient id\n",
        "summary_data = summary_data.orderBy(\"patient_ID\", \"line_start_date\")\n",
        "\n",
        "# Save summary data to a single CSV file\n",
        "summary_path = \"summary_details.csv\"\n",
        "save_as_single_csv(summary_data, summary_path)\n",
        "\n",
        "# Show summary data\n",
        "summary_data.show()"
      ],
      "metadata": {
        "id": "2rHGHQyJyoPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37ab0b1-27ad-4474-a56f-5367ea28199c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved: output_data.csv\n",
            "File saved: summary_details.csv\n",
            "+----------+--------+---------------+--------------------+---------------------+\n",
            "|patient_ID|line_num|line_start_date|        line_regimen|trgt_category_regimen|\n",
            "+----------+--------+---------------+--------------------+---------------------+\n",
            "| PATID_001|       1|     2013-12-05|PACLITAXEL + LETR...|        CHEMO + HORMO|\n",
            "| PATID_001|       2|     2014-03-30|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       3|     2014-10-25|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       4|     2015-01-30|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       5|     2015-03-19|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       6|     2015-07-14|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       7|     2015-09-08|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       8|     2015-12-29|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|       9|     2016-06-14|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|      10|     2016-10-08|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|      11|     2016-12-29|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|      12|     2017-03-19|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|      13|     2017-05-19|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|      14|     2017-07-19|PACLITAXEL + CARB...|                CHEMO|\n",
            "| PATID_001|      15|     2018-04-18|           LETROZOLE|                HORMO|\n",
            "| PATID_002|       1|     2018-04-18|         CARBOPLATIN|                CHEMO|\n",
            "| PATID_003|       1|     2018-04-18|          PACLITAXEL|                CHEMO|\n",
            "+----------+--------+---------------+--------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}